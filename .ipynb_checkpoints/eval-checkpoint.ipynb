{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c5e674d",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5a137e",
   "metadata": {},
   "source": [
    "평가 코드는 공정성을 위해 레퍼런스 코드 그대로 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2dc258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import imageio\n",
    "from glob import glob\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "from skimage.metrics import structural_similarity\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ccb919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(OUTPUT_DIR,expected_totalaug=0,test=False):\n",
    "\n",
    "    split_list = []\n",
    "    for i in (sorted(glob(OUTPUT_DIR + '/*'))):\n",
    "        split_list.append(i)\n",
    "\n",
    "    if test:\n",
    "        pass\n",
    "    else:\n",
    "        if len(split_list) != expected_totalaug:\n",
    "            print(\"Not fittable the number of augmented images !!\")\n",
    "            exit()\n",
    "        else:\n",
    "            print(\"Next Level : Split images Using Window, {} pic\".format(len(split_list)))\n",
    "\n",
    "    return split_list\n",
    "\n",
    "def resize_image_by_pil(image, scale):\n",
    "    width, height = image.shape[1], image.shape[0]\n",
    "    new_width = int(width * scale)\n",
    "    new_height = int(height * scale)\n",
    "    method = Image.BICUBIC\n",
    "\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        image = Image.fromarray(image, \"RGB\")\n",
    "        image = image.resize([new_width, new_height], resample=method)\n",
    "        image = np.asarray(image)\n",
    "    elif len(image.shape) == 3 and image.shape[2] == 4:\n",
    "        # the image may has an alpha channel\n",
    "        image = Image.fromarray(image, \"RGB\")\n",
    "        image = image.resize([new_width, new_height], resample=method)\n",
    "        image = np.asarray(image)\n",
    "    else:\n",
    "        image = Image.fromarray(image.reshape(height, width))\n",
    "        image = image.resize([new_width, new_height], resample=method)\n",
    "        image = np.asarray(image)\n",
    "        image = image.reshape(new_height, new_width, 1)\n",
    "    return image\n",
    "\n",
    "\n",
    "def save_image(filename, image, print_console=True):\n",
    "    if len(image.shape) >= 3 and image.shape[0] == 1:\n",
    "        image = image.reshape(image.shape[1], image.shape[2])\n",
    "\n",
    "    directory = os.path.dirname(filename)\n",
    "    if directory != \"\" and not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    image = image.astype(np.uint8)\n",
    "    if len(image.shape) >= 3 and image.shape[0] == 3:\n",
    "        image = Image.fromarray(image, mode=\"RGB\")\n",
    "    else:\n",
    "        image = Image.fromarray(image)\n",
    "    imageio.imwrite(filename, image)\n",
    "\n",
    "    if print_console:\n",
    "        print(\"Saved [%s]\" % filename)\n",
    "\n",
    "\n",
    "def set_image_alignment(image, alignment):\n",
    "    alignment = int(alignment)\n",
    "    width, height = image.shape[1], image.shape[0]\n",
    "    width = (width // alignment) * alignment\n",
    "    height = (height // alignment) * alignment\n",
    "\n",
    "    if image.shape[1] != width or image.shape[0] != height:\n",
    "        image = image[:height, :width, :]\n",
    "\n",
    "    if len(image.shape) >= 3 and image.shape[2] >= 4:\n",
    "        image = image[:, :, 0:3]\n",
    "\n",
    "    return image\n",
    "\n",
    "def convert_rgb_to_y(image, jpeg_mode=False, max_value=255.0):\n",
    "    if len(image.shape) <= 2 or image.shape[2] == 1:\n",
    "        return image\n",
    "\n",
    "    if jpeg_mode:\n",
    "        xform = np.array([[0.299, 0.587, 0.114]])\n",
    "        y_image = image.dot(xform.T)\n",
    "    else:\n",
    "        xform = np.array([[65.481 / 256.0, 128.553 / 256.0, 24.966 / 256.0]])\n",
    "        y_image = image.dot(xform.T) + (16.0 * max_value / 256.0)\n",
    "\n",
    "    return y_image\n",
    "\n",
    "def convert_rgb_to_ycbcr(image):\n",
    "    if len(image.shape) < 2 or image.shape[2] == 1:\n",
    "        return image\n",
    "\n",
    "    xform = np.array(\n",
    "        [[65.738 / 256.0, 129.057 / 256.0, 25.064 / 256.0],\n",
    "         [- 37.945 / 256.0, - 74.494 / 256.0, 112.439 / 256.0],\n",
    "         [112.439 / 256.0, - 94.154 / 256.0, - 18.285 / 256.0]])\n",
    "\n",
    "    ycbcr_image = image.dot(xform.T)\n",
    "    ycbcr_image[:, :, 0] += 16.0\n",
    "    ycbcr_image[:, :, [1, 2]] += 128.0\n",
    "\n",
    "    return ycbcr_image\n",
    "\n",
    "\n",
    "def convert_y_and_cbcr_to_rgb(y_image, cbcr_image):\n",
    "    print(cbcr_image.shape)\n",
    "    if len(y_image.shape) <= 2:\n",
    "        y_image = y_image.reshape[y_image.shape[0], y_image.shape[1], 1]\n",
    "\n",
    "    if len(y_image.shape) == 3 and y_image.shape[2] == 3:\n",
    "        y_image = y_image[:, :, 0:1]\n",
    "\n",
    "    ycbcr_image = np.zeros([y_image.shape[0], y_image.shape[1], 3])\n",
    "    ycbcr_image[:, :, 0] = y_image[:, :, 0]\n",
    "    ycbcr_image[:, :, 1:3] = cbcr_image[:, :, 0:2]\n",
    "\n",
    "    return convert_ycbcr_to_rgb(ycbcr_image)\n",
    "\n",
    "def convert_ycbcr_to_rgb(ycbcr_image):\n",
    "    rgb_image = np.zeros([ycbcr_image.shape[0], ycbcr_image.shape[1], 3])  # type: np.ndarray\n",
    "\n",
    "    rgb_image[:, :, 0] = ycbcr_image[:, :, 0] - 16.0\n",
    "    rgb_image[:, :, [1, 2]] = ycbcr_image[:, :, [1, 2]] - 128.0\n",
    "    xform = np.array(\n",
    "        [[298.082 / 256.0, 0, 408.583 / 256.0],\n",
    "         [298.082 / 256.0, -100.291 / 256.0, -208.120 / 256.0],\n",
    "         [298.082 / 256.0, 516.412 / 256.0, 0]])\n",
    "    rgb_image = rgb_image.dot(xform.T)\n",
    "\n",
    "    return rgb_image\n",
    "def get_split_images(image, window_size, stride=None, enable_duplicate=True):\n",
    "    if len(image.shape) == 3 and image.shape[2] == 1:\n",
    "        image = image.reshape(image.shape[0], image.shape[1])\n",
    "    #print(image.shape)\n",
    "    window_size = int(window_size)\n",
    "    size = image.itemsize\n",
    "    height, width = image.shape\n",
    "    if stride is None:\n",
    "        stride = window_size\n",
    "    else:\n",
    "        stride = int(stride)\n",
    "\n",
    "    if height < window_size or width < window_size:\n",
    "        return None\n",
    "\n",
    "    new_height = 1 + (height - window_size) // stride\n",
    "    new_width = 1 + (width - window_size) //  stride\n",
    "\n",
    "    shape = (new_height, new_width, window_size, window_size)\n",
    "    strides = size * np.array([width * stride, stride, width, 1])\n",
    "    windows = np.lib.stride_tricks.as_strided(image, shape=shape, strides=strides)\n",
    "\n",
    "\n",
    "    windows = windows.reshape(windows.shape[0] * windows.shape[1],1, windows.shape[2], windows.shape[3])\n",
    "\n",
    "    if enable_duplicate:\n",
    "        extra_windows = []\n",
    "        if (height - window_size) % stride != 0:\n",
    "            for x in range(0, width - window_size, stride):\n",
    "                extra_windows.append(image[height - window_size - 1:height - 1, x:x + window_size:])\n",
    "\n",
    "        if (width - window_size) % stride != 0:\n",
    "            for y in range(0, height - window_size, stride):\n",
    "                extra_windows.append(image[y: y + window_size, width - window_size - 1:width - 1])\n",
    "\n",
    "        if len(extra_windows) > 0:\n",
    "            org_size = windows.shape[0]\n",
    "            windows = np.resize(windows,\n",
    "                                [org_size + len(extra_windows), windows.shape[1], windows.shape[2], windows.shape[3]])\n",
    "            for i in range(len(extra_windows)):\n",
    "                extra_windows[i] = extra_windows[i].reshape([1,extra_windows[i].shape[0], extra_windows[i].shape[1]])\n",
    "                windows[org_size + i] = extra_windows[i]\n",
    "\n",
    "    return windows\n",
    "\n",
    "def GPU_AVAILABLE():\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print('Device:', device)  # 출력결과: cuda\n",
    "    print('Count of using GPUs:', torch.cuda.device_count())  # 출력결과: 2 (2, 3 두개 사용하므로)\n",
    "    print('Current cuda device:', torch.cuda.current_device())  # 출력결과: 2 (2, 3 중 앞의 GPU #2 의미)\n",
    "    return device\n",
    "\n",
    "def compute_psnr_and_ssim(image1, image2, border_size=0):\n",
    "    \"\"\"\n",
    "    Computes PSNR and SSIM index from 2 images.\n",
    "    We round it and clip to 0 - 255. Then shave 'scale' pixels from each border.\n",
    "    \"\"\"\n",
    "    if len(image1.shape) == 2:\n",
    "        image1 = image1.reshape(image1.shape[0], image1.shape[1], 1)\n",
    "    if len(image2.shape) == 2:\n",
    "        image2 = image2.reshape(image2.shape[0], image2.shape[1], 1)\n",
    "\n",
    "    if image1.shape[0] != image2.shape[0] or image1.shape[1] != image2.shape[1] or image1.shape[2] != image2.shape[2]:\n",
    "        return None\n",
    "\n",
    "    image1 = trim_image_as_file(image1)\n",
    "    image2 = trim_image_as_file(image2)\n",
    "\n",
    "    if border_size > 0:\n",
    "        image1 = image1[border_size:-border_size, border_size:-border_size, :]\n",
    "        image2 = image2[border_size:-border_size, border_size:-border_size, :]\n",
    "\n",
    "    psnr = peak_signal_noise_ratio(image1, image2, data_range=255)\n",
    "    ssim = structural_similarity(image1, image2, win_size=11, gaussian_weights=True, multichannel=True, K1=0.01, K2=0.03,\n",
    "                        sigma=1.5, data_range=255)\n",
    "    return psnr, ssim\n",
    "\n",
    "\n",
    "def trim_image_as_file(image):\n",
    "    image = np.rint(image)\n",
    "    image = np.clip(image, 0, 255)\n",
    "    if image.dtype != np.float32:\n",
    "        image = image.astype(np.float32)\n",
    "    return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ec0b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCSCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, kernel_size = 3, n_channels = 64):\n",
    "        super(DCSCN, self).__init__()\n",
    "                \n",
    "        # common layer\n",
    "        self.drop = nn.Dropout(p=0.8)\n",
    "        self.prelu = nn.PReLU() # nn.PReLU(self.conv1(x)) 방식 이용 불가. 별도 선언.\n",
    "        \n",
    "        # feature extraction layer\n",
    "        self.conv1 = nn.Conv2d(1, 196, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv2 = nn.Conv2d(196, 166, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv3 = nn.Conv2d(166, 148, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv4 = nn.Conv2d(148, 133, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv5 = nn.Conv2d(133, 120, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv6 = nn.Conv2d(120, 108, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv7 = nn.Conv2d(108, 97, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv8 = nn.Conv2d(97, 86, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv9 = nn.Conv2d(86, 76, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv10 = nn.Conv2d(76, 66, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv11 = nn.Conv2d(66, 57, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv12 = nn.Conv2d(57, 48, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        \n",
    "        ## he initialization\n",
    "        nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        nn.init.kaiming_normal_(self.conv3.weight)\n",
    "        nn.init.kaiming_normal_(self.conv4.weight)\n",
    "        nn.init.kaiming_normal_(self.conv5.weight)\n",
    "        nn.init.kaiming_normal_(self.conv6.weight)\n",
    "        nn.init.kaiming_normal_(self.conv7.weight)\n",
    "        nn.init.kaiming_normal_(self.conv8.weight)\n",
    "        nn.init.kaiming_normal_(self.conv9.weight)\n",
    "        nn.init.kaiming_normal_(self.conv10.weight)\n",
    "        nn.init.kaiming_normal_(self.conv11.weight)\n",
    "        nn.init.kaiming_normal_(self.conv12.weight)\n",
    "        \n",
    "        # reconstruction layer\n",
    "        self.A1 = nn.Conv2d(1301, 64, 1, stride = 1, bias = True)\n",
    "        self.B1 = nn.Conv2d(1301, 32, 1, stride = 1, bias = True)\n",
    "        self.B2 = nn.Conv2d(32, 32, kernel_size, stride = 1, padding=1,padding_mode=\"replicate\",bias=True)\n",
    "        \n",
    "        ## he initialization\n",
    "        nn.init.kaiming_normal_(self.A1.weight)\n",
    "        nn.init.kaiming_normal_(self.B1.weight)\n",
    "        nn.init.kaiming_normal_(self.B2.weight)\n",
    "        \n",
    "        # feature extraction layer\n",
    "        self.UPsample = nn.Conv2d(96, 2*2*96, kernel_size, stride = 1, padding =1, padding_mode = \"replicate\", bias = True)\n",
    "        self.pixelshuffle = nn.PixelShuffle(2)\n",
    "        \n",
    "        self.R = nn.Conv2d(96, 1, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        \n",
    "        ## he initialization\n",
    "        nn.init.kaiming_normal_(self.UPsample.weight)\n",
    "        nn.init.kaiming_normal_(self.R.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "                \n",
    "        # feature extraction network\n",
    "        skip1 = self.drop(self.prelu(self.conv1(x)))\n",
    "        skip2 = self.drop(self.prelu(self.conv2(skip1)))\n",
    "        skip3 = self.drop(self.prelu(self.conv3(skip2)))\n",
    "        skip4 = self.drop(self.prelu(self.conv4(skip3)))\n",
    "        skip5 = self.drop(self.prelu(self.conv5(skip4)))\n",
    "        skip6 = self.drop(self.prelu(self.conv6(skip5)))\n",
    "        skip7 = self.drop(self.prelu(self.conv7(skip6)))\n",
    "        skip8 = self.drop(self.prelu(self.conv8(skip7)))\n",
    "        skip9 = self.drop(self.prelu(self.conv9(skip8)))\n",
    "        skip10 = self.drop(self.prelu(self.conv10(skip9)))\n",
    "        skip11 = self.drop(self.prelu(self.conv11(skip10)))\n",
    "        skip12 = self.drop(self.prelu(self.conv12(skip11)))\n",
    "        \n",
    "        # reconstruction network\n",
    "        recon_input = torch.cat([skip1, skip2, skip3, skip4, skip5, skip6, skip7, skip8, skip9, skip10, skip11, skip12], dim = 1)\n",
    "        \n",
    "        A1_out = self.drop(self.prelu(self.A1(recon_input)))\n",
    "        \n",
    "        B1_out = self.drop(self.prelu(self.B1(recon_input)))\n",
    "        B2_out = self.drop(self.prelu(self.B2(B1_out)))\n",
    "        \n",
    "        recon_output = torch.cat([A1_out, B2_out], dim = 1)\n",
    "        \n",
    "        # up-sampling network\n",
    "        UPsample_out = self.drop(self.pixelshuffle(self.UPsample(recon_output)))\n",
    "        R_out = self.R(UPsample_out)\n",
    "        \n",
    "        return R_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d4846b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n"
     ]
    }
   ],
   "source": [
    "# Checking GPU Available\n",
    "\n",
    "# splited 된 그림을 보길 원하시면 batch_picture_save_flag 를 1 로 바꾸시면 됩니다.\n",
    "# 경로 : augmented_data/train_sr\n",
    "batch_picture_save_flag = 0\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)  # 출력결과: cuda\n",
    "print('Count of using GPUs:', torch.cuda.device_count())  # 출력결과: 2 (2, 3 두개 사용하므로)\n",
    "print('Current cuda device:', torch.cuda.current_device())  # 출력결과: 2 (2, 3 중 앞의 GPU #2 의미)\n",
    "\n",
    "# Configure Data Augmentation\n",
    "\n",
    "DATA_DIR = ['data/bsd200', 'data/yang91']\n",
    "OUTPUT_DIR = 'augmented_data/train_org/'\n",
    "\n",
    "# Split Parameters\n",
    "\n",
    "BICUBIC_DIR = 'augmented_data/train_sr/LRBICUBIC'\n",
    "LRX2_DIR = 'augmented_data/train_sr/LRX2'\n",
    "HR_DIR = 'augmented_data/train_sr/HR'\n",
    "\n",
    "lr_batch_size = 32\n",
    "scale = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a168c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_path,lr,bi):\n",
    "\n",
    "    device = GPU_AVAILABLE()\n",
    "\n",
    "    if len(lr.shape) == 3 and lr.shape[2] == 1:\n",
    "        lr_luma = lr.reshape(1,1,lr.shape[0], lr.shape[1])\n",
    "    if len(bi.shape) == 3 and bi.shape[2] == 1:\n",
    "        bi_luma = bi.reshape(1,1,bi.shape[0], bi.shape[1])\n",
    "\n",
    "    lr_luma = torch.FloatTensor(lr_luma).to(device)\n",
    "    bi_luma = torch.FloatTensor(bi_luma).to(device)\n",
    "    \n",
    "    load_model = torch.load(model_path).to(device)\n",
    "    load_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = load_model(lr_luma)\n",
    "        pred += bi_luma\n",
    "\n",
    "    pred = pred.detach().cpu().numpy()\n",
    "    pred = pred.reshape(pred.shape[2],pred.shape[3],1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a85cc69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(512, 512, 2)\n",
      "(512, 512, 2)\n",
      "luma_recon : 38.51050064774633 / 0.9638012975184947 luma_bicubic : 37.12439095115347 / 0.9520453248230841\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(288, 288, 2)\n",
      "(288, 288, 2)\n",
      "luma_recon : 40.95891910657626 / 0.9859568622289656 luma_bicubic : 36.73963134778147 / 0.9719822655200628\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(256, 256, 2)\n",
      "(256, 256, 2)\n",
      "luma_recon : 31.56711515586748 / 0.9608356546436907 luma_bicubic : 27.485831267157707 / 0.9150048104031713\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(280, 280, 2)\n",
      "(280, 280, 2)\n",
      "luma_recon : 35.77474722001781 / 0.8840012023219526 luma_bicubic : 34.90877421068931 / 0.8621171629666867\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(344, 228, 2)\n",
      "(344, 228, 2)\n",
      "luma_recon : 35.251708314441935 / 0.9677848159034801 luma_bicubic : 32.1838099118808 / 0.9474995647197535\n",
      "avg_psnr / avg_ssim : 36.41259808892996 / 0.9524759665233168\n"
     ]
    }
   ],
   "source": [
    "# model load\n",
    "\n",
    "load_path = 'save_model/DCSCN_V2_e100_lr0.0001.pt'\n",
    "\n",
    "# test image load\n",
    "\n",
    "test_dir = 'data/set5'\n",
    "test_list = load_img(test_dir,test=True)\n",
    "scale_factor = 2\n",
    "output_dir = 'test/'\n",
    "\n",
    "AVG_PSNR = []\n",
    "AVG_SSIM = []\n",
    "for i in test_list:\n",
    "    file_name = os.path.basename(i)\n",
    "    file_name,ext = os.path.splitext(file_name)\n",
    "\n",
    "    img = imageio.imread(i)\n",
    "    lr_img = resize_image_by_pil(img,1/scale_factor)\n",
    "    bi_img = resize_image_by_pil(lr_img,scale_factor)\n",
    "\n",
    "    y_img = convert_rgb_to_y(img)\n",
    "    y_lr_img = resize_image_by_pil(y_img,1/scale_factor)\n",
    "    y_bi_img = resize_image_by_pil(y_lr_img,scale_factor)\n",
    "    ycbcr_bi_img = convert_rgb_to_ycbcr(bi_img)\n",
    "\n",
    "    recon = test(load_path,y_lr_img,y_bi_img)\n",
    "    recon_rgb = convert_y_and_cbcr_to_rgb(recon,ycbcr_bi_img[:,:,1:3])\n",
    "\n",
    "    bicubic_rgb = convert_y_and_cbcr_to_rgb(y_bi_img,ycbcr_bi_img[:,:,1:3])\n",
    "\n",
    "    luma_psnr,luma_ssim = compute_psnr_and_ssim(y_img, recon, 2+scale_factor)\n",
    "    luma_bipsnr, luma_bissim = compute_psnr_and_ssim(y_img, y_bi_img, 0)\n",
    "\n",
    "    print(\"luma_recon : {} / {} luma_bicubic : {} / {}\".format(luma_psnr, luma_ssim, luma_bipsnr, luma_bissim))\n",
    "\n",
    "    org_name = output_dir+file_name + '_org' + ext\n",
    "    recon_save_name = output_dir+file_name + '_recon' + ext\n",
    "    bi_save_name = output_dir+file_name + '_bi' + ext\n",
    "\n",
    "    recon_rgb = np.clip(recon_rgb,0,255)\n",
    "    bicubic_rgb = np.clip(bicubic_rgb,0,255)\n",
    "\n",
    "    AVG_PSNR.append(luma_psnr)\n",
    "    AVG_SSIM.append(luma_ssim)\n",
    "\n",
    "avg_psnr = sum(AVG_PSNR)/len(AVG_PSNR)\n",
    "avg_ssim = sum(AVG_SSIM)/len(AVG_SSIM)\n",
    "\n",
    "print(\"avg_psnr / avg_ssim : {} / {}\".format(avg_psnr,avg_ssim))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
